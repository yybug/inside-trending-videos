{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. data cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#import us data \n",
    "us_df=pd.read_csv('../input/youtube-new/USvideos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df.nunique()\n",
    "#as we can see, a video can occur several times (continuous popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df['comments_disabled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df.isnull().sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the NA values\n",
    "us_df1=us_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df1.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df1['like_rate']=us_df1['likes']/(us_df1['likes']+us_df1['dislikes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df1['publish_time'] = pd.to_datetime(us_df1['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df1['p_year']=us_df1['publish_time'].dt.year\n",
    "us_df1['p_month'] = us_df1['publish_time'].dt.month\n",
    "us_df1['p_day'] = us_df1['publish_time'].dt.day\n",
    "us_df1[\"p_weekday\"]=us_df1['publish_time'].dt.weekday\n",
    "us_df1['p_time'] = us_df1['publish_time'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df1.loc[(us_df1[\"category_id\"] == 1),\"category_name\"] = 'Film and Animation'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 2),\"category_name\"] = 'Autos and Vehicles'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 10),\"category_name\"] = 'Music'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 15),\"category_name\"] = 'Pets and Animals'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 17),\"category_name\"] = 'Sports'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 18),\"category_name\"] = 'Short Movies'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 19),\"category_name\"] = 'Travel and Events'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 20),\"category_name\"] = 'Gaming'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 21),\"category_name\"] = 'Videoblogging'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 22),\"category_name\"] = 'People and Blogs'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 23),\"category_name\"] = 'Comedy'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 24),\"category_name\"] = 'Entertainment'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 25),\"category_name\"] = 'News and Politics'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 26),\"category_name\"] = 'How to and Style'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 27),\"category_name\"] = 'Education'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 28),\"category_name\"] = 'Science and Technology'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 29),\"category_name\"] = 'Non Profits and Activism'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 30),\"category_name\"] = 'Movies'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 31),\"category_name\"] = 'Anime/Animation'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 32),\"category_name\"] = 'Action/Adventure'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 33),\"category_name\"] = \"Classics\"\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 34),\"category_name\"] = 'Comedy'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 35),\"category_name\"] = 'Documentary'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 36),\"category_name\"] = 'Drama'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 37),\"category_name\"] = 'Family'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 38),\"category_name\"] = 'Foreign'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 39),\"category_name\"] = 'Horror'\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 40),\"category_name\"] = \"Sci-Fi/Fantasy\"\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 41),\"category_name\"] = \"Thriller\"\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 42),\"category_name\"] = \"Shorts\"\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 43),\"category_name\"] = \"Shows\"\n",
    "us_df1.loc[(us_df1[\"category_id\"] == 44),\"category_name\"] = \"Trailers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df2=us_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(us_df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder for convenience\n",
    "new_order=['video_id',  'title', 'channel_title', 'category_id','category_name',\n",
    "        'tags','thumbnail_link', 'comments_disabled', 'ratings_disabled',\n",
    "       'video_error_or_removed', 'description', 'trending_date','publish_time', 'p_year', 'p_month',\n",
    "       'p_weekday', 'p_time', 'views', 'likes', 'dislikes','like_rate', 'comment_count']\n",
    "us_df3=us_df2.reindex(columns=new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df3[\"video_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df3[\"appearances\"]=us_df3.groupby(\"video_id\")['video_id'].transform('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=us_df3['video_id']=='2kyS6SvSYSE'\n",
    "us_df3[con]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by video id and order by trending date\n",
    "us_df4=us_df3.groupby('video_id').apply(lambda x: x.sort_values('views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df4[[\"trending_date\",\"publish_time\",\"views\",\"appearances\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep the latest trending one\n",
    "us_df5=us_df4.drop_duplicates(['video_id'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df5 = us_df5.rename(columns={\"trending_date\": \"last_trending\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = pd.ExcelWriter('us_youtube_cleaned.xlsx', engine='xlsxwriter')\n",
    "#us_df5.to_excel(writer, sheet_name='Sheet1')\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform false to 0, true to 1 for boolean columns\n",
    "us_df5[['comments_disabled', 'ratings_disabled','video_error_or_removed']]=us_df5[['comments_disabled', 'ratings_disabled','video_error_or_removed']].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. exploratory data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 duration analysis (trending days)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(us_df5['appearances'], bins=30, normed=True, alpha=0.6,\n",
    "         histtype='stepfilled', color='green',range=[0,30],\n",
    "         edgecolor='none')\n",
    "plt.title(\"appearances distribution in US\")\n",
    "plt.savefig('p1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 views/likes distribution;\n",
    "#    views v.s. likes and dislikes (by cat?);\n",
    "#    views v.s. like rate;\n",
    "#    views by weekday (just as an example)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(np.log(us_df5['views']), bins=30, normed=True, alpha=0.6,\n",
    "         histtype='stepfilled', color='red',range=[0,20],\n",
    "         edgecolor='none')\n",
    "plt.title(\"log.views distribution in US\")\n",
    "plt.savefig('p2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "highly right skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(np.log(us_df5['likes']), bins=30, normed=True, alpha=0.6,\n",
    "         histtype='stepfilled', color='blue',range=[0,20],\n",
    "         edgecolor='none')\n",
    "plt.title(\"log.likes distribution in US\")\n",
    "plt.savefig('p3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(us_df5['likes']), np.log(us_df5['views']), 'o', color='orange',alpha=0.4)\n",
    "plt.xlabel(\"log.likes\")\n",
    "plt.ylabel(\"log.views\")\n",
    "plt.title(\"log.views v.s. log.likes in US\")\n",
    "plt.savefig('p4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 heatmap between variables (just an example)\n",
    "def heatmap(x, y, size):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Mapping from column names to integer coordinates\n",
    "    x_labels = [v for v in sorted(x.unique())]\n",
    "    y_labels = [v for v in sorted(y.unique())]\n",
    "    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n",
    "    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n",
    "    \n",
    "    size_scale = 1000\n",
    "    ax.scatter(\n",
    "        x=x.map(x_to_num), # Use mapping for x\n",
    "        y=y.map(y_to_num), # Use mapping for y\n",
    "        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n",
    "        marker='s' # Use square as scatterplot marker\n",
    "    )\n",
    "    \n",
    "    # Show column labels on the axes\n",
    "    ax.set_xticks([x_to_num[v] for v in x_labels])\n",
    "    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n",
    "    ax.set_yticks([y_to_num[v] for v in y_labels])\n",
    "    ax.set_yticklabels(y_labels)\n",
    "       \n",
    "data = us_df5\n",
    "columns = ['views', 'p_weekday', 'comments_disabled', 'ratings_disabled', 'like_rate','comment_count','appearances'] \n",
    "corr = data[columns].corr()\n",
    "corr = pd.melt(corr.reset_index(), id_vars='index') \n",
    "corr.columns = ['x', 'y', 'value']\n",
    "heatmap(\n",
    "    x=corr['x'],\n",
    "    y=corr['y'],\n",
    "    size=corr['value'].abs()\n",
    ")\n",
    "plt.savefig('p5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4 category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP: when we are done with the models, there will be more of this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#this code takes too long to run\n",
    "plt.bar(us_df3[\"category_name\"], us_df3[\"views\"], align='center', alpha=0.5)\n",
    "plt.ylabel('category')\n",
    "plt.ylabel('views')\n",
    "plt.title('views by category in US')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return word_tokenize(text)\n",
    "us_df5['tokenized_column'] = us_df5.title.apply(custom_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized_word = us_df5['tokenized_column']\n",
    "\n",
    "master_list = []\n",
    "for i in tokenized_word:\n",
    "    master_list.append(i)\n",
    "\n",
    "\n",
    "flat_list = []\n",
    "for sublist in master_list:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "\n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "\n",
    "print(listToString(flat_list))\n",
    "\n",
    "\n",
    "tokenized_word = listToString(flat_list)\n",
    "\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)\n",
    "\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re \n",
    "from nltk import sent_tokenize\n",
    "dataset = nltk.sent_tokenize(tokenized_word) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[word for word in dataset if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = us_df5.title\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = us_df5.title\n",
    "text = text.str.replace('\\d+', '')\n",
    "text = text.apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
    "text = text.str.lower().map(ps.stem)\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.toarray().tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
